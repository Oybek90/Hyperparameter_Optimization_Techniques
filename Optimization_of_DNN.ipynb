{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scorer accuracy\n",
    "score_acc = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3948, 13)\n",
      "(3948, 1)\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"STS_feat_sensor_3.csv\")\n",
    "Y = pd.read_csv(\"STS_labels_sensor_3.csv\")\n",
    "\n",
    "### Data normalization\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3158, 13)\n",
      "(790, 13)\n",
      "(3158, 1)\n",
      "(790, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=0)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    neurons = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    def nn_cl_fun():\n",
    "        opt = Adam(lr = learning_rate)\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons, input_dim=13, activation=activation))\n",
    "        nn.add(Dense(neurons, activation=activation))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,\n",
    "                         verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  epochs   | learni... |  neurons  | optimizer |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5006  \u001b[0m | \u001b[0m 5.51    \u001b[0m | \u001b[0m 113.2   \u001b[0m | \u001b[0m 54.88   \u001b[0m | \u001b[0m 0.7716  \u001b[0m | \u001b[0m 66.11   \u001b[0m | \u001b[0m 1.044   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.4997  \u001b[0m | \u001b[0m 0.2023  \u001b[0m | \u001b[0m 233.7   \u001b[0m | \u001b[0m 39.09   \u001b[0m | \u001b[0m 0.3443  \u001b[0m | \u001b[0m 198.2   \u001b[0m | \u001b[0m 1.664   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.501   \u001b[0m | \u001b[95m 0.7307  \u001b[0m | \u001b[95m 353.4   \u001b[0m | \u001b[95m 69.7    \u001b[0m | \u001b[95m 0.2815  \u001b[0m | \u001b[95m 98.58   \u001b[0m | \u001b[95m 0.8286  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.6656  \u001b[0m | \u001b[0m 464.4   \u001b[0m | \u001b[0m 83.52   \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 164.9   \u001b[0m | \u001b[0m 6.937   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.7828  \u001b[0m | \u001b[95m 5.195   \u001b[0m | \u001b[95m 422.6   \u001b[0m | \u001b[95m 53.71   \u001b[0m | \u001b[95m 0.03717 \u001b[0m | \u001b[95m 96.29   \u001b[0m | \u001b[95m 0.7373  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.4984  \u001b[0m | \u001b[0m 7.355   \u001b[0m | \u001b[0m 366.9   \u001b[0m | \u001b[0m 65.22   \u001b[0m | \u001b[0m 0.2815  \u001b[0m | \u001b[0m 199.7   \u001b[0m | \u001b[0m 0.9663  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.4994  \u001b[0m | \u001b[0m 5.539   \u001b[0m | \u001b[0m 264.8   \u001b[0m | \u001b[0m 52.4    \u001b[0m | \u001b[0m 0.7306  \u001b[0m | \u001b[0m 71.33   \u001b[0m | \u001b[0m 2.804   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.501   \u001b[0m | \u001b[0m 2.871   \u001b[0m | \u001b[0m 486.7   \u001b[0m | \u001b[0m 93.5    \u001b[0m | \u001b[0m 0.8157  \u001b[0m | \u001b[0m 16.47   \u001b[0m | \u001b[0m 6.604   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5006  \u001b[0m | \u001b[0m 8.554   \u001b[0m | \u001b[0m 419.2   \u001b[0m | \u001b[0m 58.5    \u001b[0m | \u001b[0m 0.9671  \u001b[0m | \u001b[0m 89.23   \u001b[0m | \u001b[0m 2.232   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.499   \u001b[0m | \u001b[0m 0.148   \u001b[0m | \u001b[0m 50.33   \u001b[0m | \u001b[0m 24.25   \u001b[0m | \u001b[0m 0.1367  \u001b[0m | \u001b[0m 16.34   \u001b[0m | \u001b[0m 1.585   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 4.895   \u001b[0m | \u001b[0m 117.7   \u001b[0m | \u001b[0m 34.35   \u001b[0m | \u001b[0m 0.1581  \u001b[0m | \u001b[0m 139.8   \u001b[0m | \u001b[0m 3.283   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.4984  \u001b[0m | \u001b[0m 6.914   \u001b[0m | \u001b[0m 353.0   \u001b[0m | \u001b[0m 55.3    \u001b[0m | \u001b[0m 0.5993  \u001b[0m | \u001b[0m 97.71   \u001b[0m | \u001b[0m 6.743   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.4994  \u001b[0m | \u001b[0m 1.33    \u001b[0m | \u001b[0m 467.3   \u001b[0m | \u001b[0m 59.83   \u001b[0m | \u001b[0m 0.5966  \u001b[0m | \u001b[0m 140.1   \u001b[0m | \u001b[0m 1.242   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.5652  \u001b[0m | \u001b[0m 7.782   \u001b[0m | \u001b[0m 263.4   \u001b[0m | \u001b[0m 25.55   \u001b[0m | \u001b[0m 0.3711  \u001b[0m | \u001b[0m 78.69   \u001b[0m | \u001b[0m 3.304   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.4997  \u001b[0m | \u001b[0m 1.615   \u001b[0m | \u001b[0m 116.1   \u001b[0m | \u001b[0m 95.93   \u001b[0m | \u001b[0m 0.6591  \u001b[0m | \u001b[0m 35.65   \u001b[0m | \u001b[0m 6.495   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.5595  \u001b[0m | \u001b[0m 7.576   \u001b[0m | \u001b[0m 57.33   \u001b[0m | \u001b[0m 36.29   \u001b[0m | \u001b[0m 0.8738  \u001b[0m | \u001b[0m 138.0   \u001b[0m | \u001b[0m 2.081   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.4984  \u001b[0m | \u001b[0m 6.61    \u001b[0m | \u001b[0m 328.8   \u001b[0m | \u001b[0m 36.84   \u001b[0m | \u001b[0m 0.804   \u001b[0m | \u001b[0m 21.23   \u001b[0m | \u001b[0m 2.158   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.4994  \u001b[0m | \u001b[0m 1.866   \u001b[0m | \u001b[0m 498.7   \u001b[0m | \u001b[0m 92.75   \u001b[0m | \u001b[0m 0.6797  \u001b[0m | \u001b[0m 31.89   \u001b[0m | \u001b[0m 6.706   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.4994  \u001b[0m | \u001b[0m 0.8254  \u001b[0m | \u001b[0m 334.3   \u001b[0m | \u001b[0m 92.23   \u001b[0m | \u001b[0m 0.3464  \u001b[0m | \u001b[0m 134.0   \u001b[0m | \u001b[0m 6.476   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.499   \u001b[0m | \u001b[0m 3.366   \u001b[0m | \u001b[0m 402.3   \u001b[0m | \u001b[0m 91.69   \u001b[0m | \u001b[0m 0.624   \u001b[0m | \u001b[0m 38.7    \u001b[0m | \u001b[0m 2.624   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.4984  \u001b[0m | \u001b[0m 5.723   \u001b[0m | \u001b[0m 252.4   \u001b[0m | \u001b[0m 62.58   \u001b[0m | \u001b[0m 0.3588  \u001b[0m | \u001b[0m 135.4   \u001b[0m | \u001b[0m 3.336   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.5003  \u001b[0m | \u001b[0m 4.091   \u001b[0m | \u001b[0m 91.9    \u001b[0m | \u001b[0m 53.0    \u001b[0m | \u001b[0m 0.2804  \u001b[0m | \u001b[0m 75.88   \u001b[0m | \u001b[0m 6.821   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.5006  \u001b[0m | \u001b[0m 1.94    \u001b[0m | \u001b[0m 359.8   \u001b[0m | \u001b[0m 22.54   \u001b[0m | \u001b[0m 0.837   \u001b[0m | \u001b[0m 143.3   \u001b[0m | \u001b[0m 6.762   \u001b[0m |\n",
      "| \u001b[95m 24      \u001b[0m | \u001b[95m 0.804   \u001b[0m | \u001b[95m 5.326   \u001b[0m | \u001b[95m 136.3   \u001b[0m | \u001b[95m 77.54   \u001b[0m | \u001b[95m 0.04056 \u001b[0m | \u001b[95m 89.55   \u001b[0m | \u001b[95m 1.969   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.4997  \u001b[0m | \u001b[0m 0.9562  \u001b[0m | \u001b[0m 236.6   \u001b[0m | \u001b[0m 87.25   \u001b[0m | \u001b[0m 0.1193  \u001b[0m | \u001b[0m 197.5   \u001b[0m | \u001b[0m 1.633   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.5728  \u001b[0m | \u001b[0m 7.588   \u001b[0m | \u001b[0m 52.49   \u001b[0m | \u001b[0m 72.25   \u001b[0m | \u001b[0m 0.4108  \u001b[0m | \u001b[0m 47.72   \u001b[0m | \u001b[0m 4.56    \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.724   \u001b[0m | \u001b[0m 159.4   \u001b[0m | \u001b[0m 62.06   \u001b[0m | \u001b[0m 0.9792  \u001b[0m | \u001b[0m 152.3   \u001b[0m | \u001b[0m 4.245   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.501   \u001b[0m | \u001b[0m 1.738   \u001b[0m | \u001b[0m 510.3   \u001b[0m | \u001b[0m 48.97   \u001b[0m | \u001b[0m 0.7989  \u001b[0m | \u001b[0m 88.06   \u001b[0m | \u001b[0m 6.493   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.4997  \u001b[0m | \u001b[0m 8.671   \u001b[0m | \u001b[0m 408.8   \u001b[0m | \u001b[0m 51.03   \u001b[0m | \u001b[0m 0.8384  \u001b[0m | \u001b[0m 106.6   \u001b[0m | \u001b[0m 6.071   \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Set paramaters\n",
    "params_nn ={\n",
    "    'neurons': (10, 200),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(32, 512),\n",
    "    'epochs':(20, 100)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=111)\n",
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'selu',\n",
       " 'batch_size': 136.32580217257484,\n",
       " 'epochs': 77.53809882694163,\n",
       " 'learning_rate': 0.04056128946327648,\n",
       " 'neurons': 89.55451097809664,\n",
       " 'optimizer': 1.968575834097605}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_nn_ = nn_bo.max['params']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo2(neurons, activation, optimizer, learning_rate, batch_size, epochs,\n",
    "              layers1, layers2, normalization, dropout, dropout_rate):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    neurons = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    optimizer = optimizerD[optimizerL[round(optimizer)]]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    layers1 = round(layers1)\n",
    "    layers2 = round(layers2)\n",
    "    def nn_cl_fun():\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons, input_dim=13, activation=activation))\n",
    "        if normalization > 0.5:\n",
    "            nn.add(BatchNormalization())\n",
    "        for i in range(layers1):\n",
    "            nn.add(Dense(neurons, activation=activation))\n",
    "        if dropout > 0.5:\n",
    "            nn.add(Dropout(dropout_rate, seed=123))\n",
    "        for i in range(layers2):\n",
    "            nn.add(Dense(neurons, activation=activation))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, X_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  dropout  | dropou... |  epochs   |  layers1  |  layers2  | learni... |  neurons  | normal... | optimizer |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5016  \u001b[0m | \u001b[0m 5.51    \u001b[0m | \u001b[0m 113.2   \u001b[0m | \u001b[0m 0.4361  \u001b[0m | \u001b[0m 0.2308  \u001b[0m | \u001b[0m 43.63   \u001b[0m | \u001b[0m 1.447   \u001b[0m | \u001b[0m 1.067   \u001b[0m | \u001b[0m 0.426   \u001b[0m | \u001b[0m 55.35   \u001b[0m | \u001b[0m 0.3377  \u001b[0m | \u001b[0m 6.935   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.5025  \u001b[0m | \u001b[95m 2.14    \u001b[0m | \u001b[95m 70.97   \u001b[0m | \u001b[95m 0.6696  \u001b[0m | \u001b[95m 0.1864  \u001b[0m | \u001b[95m 41.94   \u001b[0m | \u001b[95m 2.399   \u001b[0m | \u001b[95m 1.355   \u001b[0m | \u001b[95m 0.08322 \u001b[0m | \u001b[95m 181.1   \u001b[0m | \u001b[95m 0.794   \u001b[0m | \u001b[95m 5.884   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.4984  \u001b[0m | \u001b[0m 7.337   \u001b[0m | \u001b[0m 507.7   \u001b[0m | \u001b[0m 0.5773  \u001b[0m | \u001b[0m 0.2441  \u001b[0m | \u001b[0m 53.71   \u001b[0m | \u001b[0m 1.082   \u001b[0m | \u001b[0m 2.362   \u001b[0m | \u001b[0m 0.1143  \u001b[0m | \u001b[0m 165.3   \u001b[0m | \u001b[0m 0.6977  \u001b[0m | \u001b[0m 3.957   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 2.468   \u001b[0m | \u001b[0m 511.3   \u001b[0m | \u001b[0m 0.138   \u001b[0m | \u001b[0m 0.1846  \u001b[0m | \u001b[0m 58.8    \u001b[0m | \u001b[0m 2.215   \u001b[0m | \u001b[0m 3.184   \u001b[0m | \u001b[0m 0.3296  \u001b[0m | \u001b[0m 86.1    \u001b[0m | \u001b[0m 0.319   \u001b[0m | \u001b[0m 6.631   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.5548  \u001b[0m | \u001b[95m 8.268   \u001b[0m | \u001b[95m 422.6   \u001b[0m | \u001b[95m 0.03408 \u001b[0m | \u001b[95m 0.283   \u001b[0m | \u001b[95m 96.04   \u001b[0m | \u001b[95m 3.42    \u001b[0m | \u001b[95m 2.444   \u001b[0m | \u001b[95m 0.9671  \u001b[0m | \u001b[95m 89.23   \u001b[0m | \u001b[95m 0.3188  \u001b[0m | \u001b[95m 0.1151  \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.817   \u001b[0m | \u001b[95m 0.3436  \u001b[0m | \u001b[95m 57.48   \u001b[0m | \u001b[95m 0.128   \u001b[0m | \u001b[95m 0.01001 \u001b[0m | \u001b[95m 38.11   \u001b[0m | \u001b[95m 2.632   \u001b[0m | \u001b[95m 1.536   \u001b[0m | \u001b[95m 0.1876  \u001b[0m | \u001b[95m 38.43   \u001b[0m | \u001b[95m 0.683   \u001b[0m | \u001b[95m 3.283   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.4984  \u001b[0m | \u001b[0m 6.914   \u001b[0m | \u001b[0m 353.0   \u001b[0m | \u001b[0m 0.4413  \u001b[0m | \u001b[0m 0.1786  \u001b[0m | \u001b[0m 56.93   \u001b[0m | \u001b[0m 3.89    \u001b[0m | \u001b[0m 1.443   \u001b[0m | \u001b[0m 0.9077  \u001b[0m | \u001b[0m 104.6   \u001b[0m | \u001b[0m 0.5925  \u001b[0m | \u001b[0m 4.793   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6153  \u001b[0m | \u001b[0m 1.597   \u001b[0m | \u001b[0m 447.0   \u001b[0m | \u001b[0m 0.4821  \u001b[0m | \u001b[0m 0.0208  \u001b[0m | \u001b[0m 49.18   \u001b[0m | \u001b[0m 2.085   \u001b[0m | \u001b[0m 2.416   \u001b[0m | \u001b[0m 0.1877  \u001b[0m | \u001b[0m 43.3    \u001b[0m | \u001b[0m 0.9491  \u001b[0m | \u001b[0m 4.59    \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.4997  \u001b[0m | \u001b[0m 1.215   \u001b[0m | \u001b[0m 477.3   \u001b[0m | \u001b[0m 0.8418  \u001b[0m | \u001b[0m 0.01583 \u001b[0m | \u001b[0m 36.29   \u001b[0m | \u001b[0m 3.618   \u001b[0m | \u001b[0m 3.022   \u001b[0m | \u001b[0m 0.3043  \u001b[0m | \u001b[0m 149.5   \u001b[0m | \u001b[0m 0.6183  \u001b[0m | \u001b[0m 1.473   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.4984  \u001b[0m | \u001b[0m 7.219   \u001b[0m | \u001b[0m 60.37   \u001b[0m | \u001b[0m 0.3082  \u001b[0m | \u001b[0m 0.06221 \u001b[0m | \u001b[0m 97.78   \u001b[0m | \u001b[0m 3.728   \u001b[0m | \u001b[0m 3.029   \u001b[0m | \u001b[0m 0.124   \u001b[0m | \u001b[0m 192.0   \u001b[0m | \u001b[0m 0.09171 \u001b[0m | \u001b[0m 4.409   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.5387  \u001b[0m | \u001b[0m 8.126   \u001b[0m | \u001b[0m 195.1   \u001b[0m | \u001b[0m 0.6528  \u001b[0m | \u001b[0m 0.2775  \u001b[0m | \u001b[0m 49.92   \u001b[0m | \u001b[0m 3.314   \u001b[0m | \u001b[0m 3.688   \u001b[0m | \u001b[0m 0.624   \u001b[0m | \u001b[0m 38.7    \u001b[0m | \u001b[0m 0.3749  \u001b[0m | \u001b[0m 4.451   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.5016  \u001b[0m | \u001b[0m 4.132   \u001b[0m | \u001b[0m 287.5   \u001b[0m | \u001b[0m 0.3523  \u001b[0m | \u001b[0m 0.198   \u001b[0m | \u001b[0m 58.12   \u001b[0m | \u001b[0m 2.364   \u001b[0m | \u001b[0m 1.374   \u001b[0m | \u001b[0m 0.4183  \u001b[0m | \u001b[0m 61.89   \u001b[0m | \u001b[0m 0.3467  \u001b[0m | \u001b[0m 6.821   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.501   \u001b[0m | \u001b[0m 1.94    \u001b[0m | \u001b[0m 359.8   \u001b[0m | \u001b[0m 0.03181 \u001b[0m | \u001b[0m 0.2506  \u001b[0m | \u001b[0m 76.13   \u001b[0m | \u001b[0m 3.898   \u001b[0m | \u001b[0m 2.775   \u001b[0m | \u001b[0m 0.2252  \u001b[0m | \u001b[0m 146.7   \u001b[0m | \u001b[0m 0.03087 \u001b[0m | \u001b[0m 2.931   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.5003  \u001b[0m | \u001b[0m 2.531   \u001b[0m | \u001b[0m 83.0    \u001b[0m | \u001b[0m 0.4263  \u001b[0m | \u001b[0m 0.2522  \u001b[0m | \u001b[0m 28.83   \u001b[0m | \u001b[0m 3.96    \u001b[0m | \u001b[0m 1.7     \u001b[0m | \u001b[0m 0.7242  \u001b[0m | \u001b[0m 135.6   \u001b[0m | \u001b[0m 0.07776 \u001b[0m | \u001b[0m 4.881   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.5079  \u001b[0m | \u001b[0m 2.388   \u001b[0m | \u001b[0m 464.9   \u001b[0m | \u001b[0m 0.8183  \u001b[0m | \u001b[0m 0.1198  \u001b[0m | \u001b[0m 85.62   \u001b[0m | \u001b[0m 1.593   \u001b[0m | \u001b[0m 2.568   \u001b[0m | \u001b[0m 0.4184  \u001b[0m | \u001b[0m 185.9   \u001b[0m | \u001b[0m 0.8254  \u001b[0m | \u001b[0m 3.507   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.5314  \u001b[0m | \u001b[0m 1.051   \u001b[0m | \u001b[0m 37.56   \u001b[0m | \u001b[0m 0.9132  \u001b[0m | \u001b[0m 0.1537  \u001b[0m | \u001b[0m 87.45   \u001b[0m | \u001b[0m 1.285   \u001b[0m | \u001b[0m 3.41    \u001b[0m | \u001b[0m 0.07161 \u001b[0m | \u001b[0m 130.7   \u001b[0m | \u001b[0m 0.9688  \u001b[0m | \u001b[0m 2.782   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.5006  \u001b[0m | \u001b[0m 5.936   \u001b[0m | \u001b[0m 135.1   \u001b[0m | \u001b[0m 0.8899  \u001b[0m | \u001b[0m 0.296   \u001b[0m | \u001b[0m 79.09   \u001b[0m | \u001b[0m 2.924   \u001b[0m | \u001b[0m 1.756   \u001b[0m | \u001b[0m 0.4811  \u001b[0m | \u001b[0m 60.93   \u001b[0m | \u001b[0m 0.8683  \u001b[0m | \u001b[0m 1.868   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.5003  \u001b[0m | \u001b[0m 8.757   \u001b[0m | \u001b[0m 134.5   \u001b[0m | \u001b[0m 0.2978  \u001b[0m | \u001b[0m 0.221   \u001b[0m | \u001b[0m 21.03   \u001b[0m | \u001b[0m 1.091   \u001b[0m | \u001b[0m 3.201   \u001b[0m | \u001b[0m 0.5033  \u001b[0m | \u001b[0m 51.43   \u001b[0m | \u001b[0m 0.00893 \u001b[0m | \u001b[0m 5.955   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.6482  \u001b[0m | \u001b[0m 4.828   \u001b[0m | \u001b[0m 379.3   \u001b[0m | \u001b[0m 0.6616  \u001b[0m | \u001b[0m 0.2516  \u001b[0m | \u001b[0m 51.06   \u001b[0m | \u001b[0m 2.279   \u001b[0m | \u001b[0m 3.484   \u001b[0m | \u001b[0m 0.4743  \u001b[0m | \u001b[0m 165.8   \u001b[0m | \u001b[0m 0.01418 \u001b[0m | \u001b[0m 2.777   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.499   \u001b[0m | \u001b[0m 1.155   \u001b[0m | \u001b[0m 88.68   \u001b[0m | \u001b[0m 0.206   \u001b[0m | \u001b[0m 0.2243  \u001b[0m | \u001b[0m 94.41   \u001b[0m | \u001b[0m 2.142   \u001b[0m | \u001b[0m 2.382   \u001b[0m | \u001b[0m 0.8746  \u001b[0m | \u001b[0m 164.8   \u001b[0m | \u001b[0m 0.02497 \u001b[0m | \u001b[0m 6.111   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.4972  \u001b[0m | \u001b[0m 5.441   \u001b[0m | \u001b[0m 279.9   \u001b[0m | \u001b[0m 0.5893  \u001b[0m | \u001b[0m 0.2399  \u001b[0m | \u001b[0m 33.86   \u001b[0m | \u001b[0m 1.561   \u001b[0m | \u001b[0m 1.775   \u001b[0m | \u001b[0m 0.06056 \u001b[0m | \u001b[0m 115.0   \u001b[0m | \u001b[0m 0.3518  \u001b[0m | \u001b[0m 6.419   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.7682  \u001b[0m | \u001b[0m 4.289   \u001b[0m | \u001b[0m 82.18   \u001b[0m | \u001b[0m 0.1525  \u001b[0m | \u001b[0m 0.08206 \u001b[0m | \u001b[0m 82.52   \u001b[0m | \u001b[0m 2.179   \u001b[0m | \u001b[0m 3.397   \u001b[0m | \u001b[0m 0.4387  \u001b[0m | \u001b[0m 25.5    \u001b[0m | \u001b[0m 0.01064 \u001b[0m | \u001b[0m 3.016   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.5564  \u001b[0m | \u001b[0m 5.966   \u001b[0m | \u001b[0m 279.3   \u001b[0m | \u001b[0m 0.5801  \u001b[0m | \u001b[0m 0.1479  \u001b[0m | \u001b[0m 79.24   \u001b[0m | \u001b[0m 3.368   \u001b[0m | \u001b[0m 3.343   \u001b[0m | \u001b[0m 0.1363  \u001b[0m | \u001b[0m 188.6   \u001b[0m | \u001b[0m 0.8777  \u001b[0m | \u001b[0m 4.897   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.5098  \u001b[0m | \u001b[0m 8.432   \u001b[0m | \u001b[0m 355.4   \u001b[0m | \u001b[0m 0.5944  \u001b[0m | \u001b[0m 0.1035  \u001b[0m | \u001b[0m 26.69   \u001b[0m | \u001b[0m 2.738   \u001b[0m | \u001b[0m 1.053   \u001b[0m | \u001b[0m 0.5569  \u001b[0m | \u001b[0m 130.2   \u001b[0m | \u001b[0m 0.6784  \u001b[0m | \u001b[0m 1.194   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.5025  \u001b[0m | \u001b[0m 5.194   \u001b[0m | \u001b[0m 130.9   \u001b[0m | \u001b[0m 0.2515  \u001b[0m | \u001b[0m 0.2908  \u001b[0m | \u001b[0m 91.73   \u001b[0m | \u001b[0m 1.369   \u001b[0m | \u001b[0m 3.644   \u001b[0m | \u001b[0m 0.9485  \u001b[0m | \u001b[0m 97.38   \u001b[0m | \u001b[0m 0.413   \u001b[0m | \u001b[0m 4.04    \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.5611  \u001b[0m | \u001b[0m 1.144   \u001b[0m | \u001b[0m 52.36   \u001b[0m | \u001b[0m 0.8838  \u001b[0m | \u001b[0m 0.2287  \u001b[0m | \u001b[0m 59.78   \u001b[0m | \u001b[0m 1.393   \u001b[0m | \u001b[0m 1.476   \u001b[0m | \u001b[0m 0.8371  \u001b[0m | \u001b[0m 11.16   \u001b[0m | \u001b[0m 0.9342  \u001b[0m | \u001b[0m 5.155   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.4975  \u001b[0m | \u001b[0m 2.958   \u001b[0m | \u001b[0m 446.0   \u001b[0m | \u001b[0m 0.6777  \u001b[0m | \u001b[0m 0.2451  \u001b[0m | \u001b[0m 51.99   \u001b[0m | \u001b[0m 3.283   \u001b[0m | \u001b[0m 2.988   \u001b[0m | \u001b[0m 0.5303  \u001b[0m | \u001b[0m 41.13   \u001b[0m | \u001b[0m 0.9532  \u001b[0m | \u001b[0m 1.976   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.4994  \u001b[0m | \u001b[0m 1.36    \u001b[0m | \u001b[0m 364.3   \u001b[0m | \u001b[0m 0.5885  \u001b[0m | \u001b[0m 0.1415  \u001b[0m | \u001b[0m 68.2    \u001b[0m | \u001b[0m 1.031   \u001b[0m | \u001b[0m 1.65    \u001b[0m | \u001b[0m 0.6129  \u001b[0m | \u001b[0m 115.8   \u001b[0m | \u001b[0m 0.1266  \u001b[0m | \u001b[0m 5.56    \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.4984  \u001b[0m | \u001b[0m 7.191   \u001b[0m | \u001b[0m 491.7   \u001b[0m | \u001b[0m 0.04053 \u001b[0m | \u001b[0m 0.2721  \u001b[0m | \u001b[0m 78.01   \u001b[0m | \u001b[0m 1.611   \u001b[0m | \u001b[0m 3.194   \u001b[0m | \u001b[0m 0.2568  \u001b[0m | \u001b[0m 79.75   \u001b[0m | \u001b[0m 0.2389  \u001b[0m | \u001b[0m 4.9     \u001b[0m |\n",
      "=============================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "params_nn2 ={\n",
    "    'neurons': (10, 200),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(32, 512),\n",
    "    'epochs':(20, 100),\n",
    "    'layers1':(1,4),\n",
    "    'layers2':(1,4),\n",
    "    'normalization':(0,1),\n",
    "    'dropout':(0,1),\n",
    "    'dropout_rate':(0,0.3)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\n",
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'batch_size': 57,\n",
       " 'dropout': 0.1279608146375314,\n",
       " 'dropout_rate': 0.010014947837176436,\n",
       " 'epochs': 38,\n",
       " 'layers1': 3,\n",
       " 'layers2': 2,\n",
       " 'learning_rate': 0.18755133602737314,\n",
       " 'neurons': 38,\n",
       " 'normalization': 0.6830131255680931,\n",
       " 'optimizer': <tensorflow.python.keras.optimizer_v2.adadelta.Adadelta at 0x209810f3d88>}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_nn_ = nn_bo.max['params']\n",
    "learning_rate = params_nn_['learning_rate']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
    "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
    "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
    "params_nn_['layers1'] = round(params_nn_['layers1'])\n",
    "params_nn_['layers2'] = round(params_nn_['layers2'])\n",
    "params_nn_['neurons'] = round(params_nn_['neurons'])\n",
    "optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\n",
    "optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "             'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "             'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "             'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "params_nn_['optimizer'] = optimizerD[optimizerL[round(params_nn_['optimizer'])]]\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3158 samples, validate on 790 samples\n",
      "Epoch 1/38\n",
      "3158/3158 [==============================] - 0s 155us/step - loss: 0.6739 - accuracy: 0.5484 - val_loss: 0.6897 - val_accuracy: 0.5101\n",
      "Epoch 2/38\n",
      "3158/3158 [==============================] - 0s 77us/step - loss: 0.6204 - accuracy: 0.7017 - val_loss: 0.6864 - val_accuracy: 0.5139\n",
      "Epoch 3/38\n",
      "3158/3158 [==============================] - 0s 75us/step - loss: 0.5343 - accuracy: 0.7498 - val_loss: 0.6479 - val_accuracy: 0.6354\n",
      "Epoch 4/38\n",
      "3158/3158 [==============================] - 0s 76us/step - loss: 0.4759 - accuracy: 0.7726 - val_loss: 0.5963 - val_accuracy: 0.7253\n",
      "Epoch 5/38\n",
      "3158/3158 [==============================] - 0s 78us/step - loss: 0.4468 - accuracy: 0.7932 - val_loss: 0.5601 - val_accuracy: 0.7304\n",
      "Epoch 6/38\n",
      "3158/3158 [==============================] - 0s 79us/step - loss: 0.4395 - accuracy: 0.7907 - val_loss: 0.5591 - val_accuracy: 0.7177\n",
      "Epoch 7/38\n",
      "3158/3158 [==============================] - 0s 80us/step - loss: 0.4314 - accuracy: 0.8002 - val_loss: 0.5022 - val_accuracy: 0.7709\n",
      "Epoch 8/38\n",
      "3158/3158 [==============================] - 0s 78us/step - loss: 0.4248 - accuracy: 0.7999 - val_loss: 0.5134 - val_accuracy: 0.7519\n",
      "Epoch 9/38\n",
      "3158/3158 [==============================] - 0s 79us/step - loss: 0.4229 - accuracy: 0.8056 - val_loss: 0.4457 - val_accuracy: 0.8165\n",
      "Epoch 10/38\n",
      "3158/3158 [==============================] - 0s 79us/step - loss: 0.4232 - accuracy: 0.8005 - val_loss: 0.4682 - val_accuracy: 0.7835\n",
      "Epoch 11/38\n",
      "3158/3158 [==============================] - 0s 79us/step - loss: 0.4209 - accuracy: 0.8030 - val_loss: 0.4236 - val_accuracy: 0.8127\n",
      "Epoch 12/38\n",
      "3158/3158 [==============================] - 0s 82us/step - loss: 0.4174 - accuracy: 0.8037 - val_loss: 0.4090 - val_accuracy: 0.8215\n",
      "Epoch 13/38\n",
      "3158/3158 [==============================] - 0s 79us/step - loss: 0.4106 - accuracy: 0.8144 - val_loss: 0.4237 - val_accuracy: 0.8152\n",
      "Epoch 14/38\n",
      "3158/3158 [==============================] - 0s 78us/step - loss: 0.4102 - accuracy: 0.8106 - val_loss: 0.4127 - val_accuracy: 0.8291\n",
      "Epoch 15/38\n",
      "3158/3158 [==============================] - 0s 79us/step - loss: 0.4071 - accuracy: 0.8182 - val_loss: 0.4013 - val_accuracy: 0.8278\n",
      "Epoch 16/38\n",
      "3158/3158 [==============================] - 0s 78us/step - loss: 0.4083 - accuracy: 0.8097 - val_loss: 0.4020 - val_accuracy: 0.8203\n",
      "Epoch 17/38\n",
      "3158/3158 [==============================] - 0s 79us/step - loss: 0.4090 - accuracy: 0.8167 - val_loss: 0.4149 - val_accuracy: 0.8203\n",
      "Epoch 18/38\n",
      "3158/3158 [==============================] - 0s 76us/step - loss: 0.4056 - accuracy: 0.8163 - val_loss: 0.3955 - val_accuracy: 0.8316\n",
      "Epoch 19/38\n",
      "3158/3158 [==============================] - 0s 78us/step - loss: 0.4025 - accuracy: 0.8157 - val_loss: 0.3947 - val_accuracy: 0.8342\n",
      "Epoch 20/38\n",
      "3158/3158 [==============================] - 0s 76us/step - loss: 0.4024 - accuracy: 0.8135 - val_loss: 0.3981 - val_accuracy: 0.8354\n",
      "Epoch 21/38\n",
      "3158/3158 [==============================] - 0s 77us/step - loss: 0.4066 - accuracy: 0.8113 - val_loss: 0.4083 - val_accuracy: 0.8253\n",
      "Epoch 22/38\n",
      "3158/3158 [==============================] - 0s 77us/step - loss: 0.4001 - accuracy: 0.8211 - val_loss: 0.3917 - val_accuracy: 0.8380\n",
      "Epoch 23/38\n",
      "3158/3158 [==============================] - 0s 75us/step - loss: 0.3998 - accuracy: 0.8160 - val_loss: 0.3894 - val_accuracy: 0.8380\n",
      "Epoch 24/38\n",
      "3158/3158 [==============================] - 0s 77us/step - loss: 0.4079 - accuracy: 0.8094 - val_loss: 0.3934 - val_accuracy: 0.8278\n",
      "Epoch 25/38\n",
      "3158/3158 [==============================] - 0s 81us/step - loss: 0.3981 - accuracy: 0.8179 - val_loss: 0.3891 - val_accuracy: 0.8405\n",
      "Epoch 26/38\n",
      "3158/3158 [==============================] - 0s 77us/step - loss: 0.3987 - accuracy: 0.8170 - val_loss: 0.3889 - val_accuracy: 0.8367\n",
      "Epoch 27/38\n",
      "3158/3158 [==============================] - 0s 78us/step - loss: 0.3995 - accuracy: 0.8129 - val_loss: 0.3936 - val_accuracy: 0.8367\n",
      "Epoch 28/38\n",
      "3158/3158 [==============================] - 0s 78us/step - loss: 0.3994 - accuracy: 0.8176 - val_loss: 0.3835 - val_accuracy: 0.8367\n",
      "Epoch 29/38\n",
      "3158/3158 [==============================] - 0s 77us/step - loss: 0.3964 - accuracy: 0.8167 - val_loss: 0.3892 - val_accuracy: 0.8367\n",
      "Epoch 30/38\n",
      "3158/3158 [==============================] - 0s 78us/step - loss: 0.3999 - accuracy: 0.8163 - val_loss: 0.3866 - val_accuracy: 0.8354\n",
      "Epoch 31/38\n",
      "3158/3158 [==============================] - 0s 80us/step - loss: 0.3919 - accuracy: 0.8176 - val_loss: 0.3892 - val_accuracy: 0.8354\n",
      "Epoch 32/38\n",
      "3158/3158 [==============================] - 0s 79us/step - loss: 0.3890 - accuracy: 0.8167 - val_loss: 0.3922 - val_accuracy: 0.8418\n",
      "Epoch 33/38\n",
      "3158/3158 [==============================] - 0s 78us/step - loss: 0.3990 - accuracy: 0.8141 - val_loss: 0.3932 - val_accuracy: 0.8392\n",
      "Epoch 34/38\n",
      "3158/3158 [==============================] - 0s 75us/step - loss: 0.3936 - accuracy: 0.8205 - val_loss: 0.3917 - val_accuracy: 0.8380\n",
      "Epoch 35/38\n",
      "3158/3158 [==============================] - 0s 79us/step - loss: 0.3905 - accuracy: 0.8198 - val_loss: 0.3902 - val_accuracy: 0.8405\n",
      "Epoch 36/38\n",
      "3158/3158 [==============================] - 0s 79us/step - loss: 0.4005 - accuracy: 0.8205 - val_loss: 0.3888 - val_accuracy: 0.8392\n",
      "Epoch 37/38\n",
      "3158/3158 [==============================] - 0s 76us/step - loss: 0.3987 - accuracy: 0.8113 - val_loss: 0.3999 - val_accuracy: 0.8392\n",
      "Epoch 38/38\n",
      "3158/3158 [==============================] - 0s 78us/step - loss: 0.3924 - accuracy: 0.8220 - val_loss: 0.3868 - val_accuracy: 0.8380\n"
     ]
    }
   ],
   "source": [
    "# Fitting Neural Network\n",
    "def nn_cl_fun():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(params_nn_['neurons'], input_dim=13, activation=params_nn_['activation']))\n",
    "    if params_nn_['normalization'] > 0.5:\n",
    "        nn.add(BatchNormalization())\n",
    "    for i in range(params_nn_['layers1']):\n",
    "        nn.add(Dense(params_nn_['neurons'], activation=params_nn_['activation']))\n",
    "    if params_nn_['dropout'] > 0.5:\n",
    "        nn.add(Dropout(params_nn_['dropout_rate'], seed=123))\n",
    "    for i in range(params_nn_['layers2']):\n",
    "        nn.add(Dense(params_nn_['neurons'], activation=params_nn_['activation']))\n",
    "    nn.add(Dense(1, activation='sigmoid'))\n",
    "    nn.compile(loss='binary_crossentropy', optimizer=params_nn_['optimizer'], metrics=['accuracy'])\n",
    "    return nn\n",
    "es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "nn = KerasClassifier(build_fn=nn_cl_fun, epochs=params_nn_['epochs'], batch_size=params_nn_['batch_size'],\n",
    "                         verbose=0)\n",
    "history = nn.fit(X_train, y_train, validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 83.79746675491333 %\n"
     ]
    }
   ],
   "source": [
    "test_acc = nn.score(X_test, y_test)\n",
    "print('\\nTest accuracy:', test_acc*100, \"%\")\n",
    "\n",
    "### first result 78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
